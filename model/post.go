package model

import (
	"time"

	"github.com/lib/pq"
	"gorm.io/gorm"
)

/*

Post is a piece of news crawler fetched

Id: primary key, use to identify a sub-source
CreatedAt: time when entity is created
DeletedAt: time when entity is deleted

Title: post's title in plain text
Content: post's content in plain text
SourceID:
Source: source website for example "twitter", "weibo", "Caixin",  "belongs-to" relation
SubSourceID:
SubSource: for example a twitter user, weibo user, sub channel in Caixin etc., "belongs-to" relation

SharedFromPostID:
SharedFromPost:
		if the post is a user shared(like re-twitt) post, set this as the Post originally shared. Support multi-level sharing.
		also if the post is shared:
			Source is a FAKE one representing "shared".
				Name is "分享"
			SubSource representing "user".
				CreatorID is user_id
				Creator is User
				Name is user's name
				ExternalIdentifier is empty


SavedByUser: mark when user save the post, "many-to-many" relation
PublishedFeeds: feeds that this post published to, "many-to-many" relation

Cursor: The auto-inc global-unique index to keep the relative order of posts

ImageUrlsInJson: Urls for the attached images, in json format string

FileUrlsInJson: Urls for the attached files, in json format string

CrawledAt: The time stamp when crawler generate the post

OriginUrl: The origin_url is the url of crawled website example: "http://companies.caixin.com/2021-04-10/101688620.html"

ContentGeneratedAt: The actual time content is generated by 3rd party website

InSharingChain: If this post is some other post retweets.
	the in-chain posts will not be published as an independent post in a feed, however,
	however, its content will be used to publish/match its root post in the chain.

DeduplicateId: A hash to deduplicate across posts, generated by crawler

SemanticHashing: A hash with the property that similar content will be hashed
into near neighbor in Hamming space. It is a 128 bit binary string.
*/

type Post struct {
	Id                 string `gorm:"primaryKey"`
	CreatedAt          time.Time
	DeletedAt          gorm.DeletedAt
	Title              string    `json:"title"`
	Content            string    `json:"content"`
	SubSourceID        string    `gorm:"constraint:OnUpdate:CASCADE,OnDelete:SET NULL;"`
	SubSource          SubSource `gorm:"constraint:OnUpdate:CASCADE,OnDelete:SET NULL;"`
	SharedFromPostID   *string   `json:"shared_from_post_id"`
	SharedFromPost     *Post     `json:"shared_from_post"`
	SavedByUser        []*User   `json:"saved_by_user" gorm:"many2many;"`
	PublishedFeeds     []*Feed   `json:"published_feeds" gorm:"many2many:post_feed_publishes;constraint:OnDelete:CASCADE;"`
	Cursor             int32     `gorm:"autoIncrement"`
	CrawledAt          time.Time `json:"crawled_at"`
	OriginUrl          string    `json:"origin_url"`
	ContentGeneratedAt time.Time `json:"content_generated_at"`
	InSharingChain     bool      `json:"in_sharing_chain"`

	// A post could be within a reply thread, this field stored all ancestors of
	// this Post, in a chronological order. Under the hood this is tracked via a
	// many to many table of Post.
	// The first Post in ReplyThread is a post that doesn't reply to anything (
	// root post in this thread.)
	// Note that we explicitly not storing this thread in a recursive way similar
	// to SharedFromPost, mostly because GraphQL & GORM doesn't have a way of
	// recursive loading. This would be fine for SharedFromPost because we limit
	// it to be only 2 layers but would make the loading for thread impossible
	// due to the fact that we don't know how long is this thread.
	ReplyThread []*Post `json:"reply_thread" gorm:"many2many:reply_thread;constraint:OnDelete:CASCADE;"`

	// TODO: convert json to array when serve graphql API and ingest from crawler
	ImageUrls pq.StringArray `gorm:"type:TEXT[]" json:"image_urls"`
	FileUrls  pq.StringArray `gorm:"type:TEXT[]" json:"file_urls"`

	DeduplicateId   string `json:"deduplicate_id"`
	SemanticHashing string `json:"semantic_hashing"`
	Tag             string `json:"tag"`
	IsRead          bool   `json:"is_read" gorm:"-"`
}
