syntax = "proto3";
package protocol;

import "google/protobuf/timestamp.proto";

option go_package = "github.com/Luismorlan/newsmux/publisher/protocol";

message KeyValuePair {
  string key = 1;
  string value = 2;
}

message PanopticJob {
  // Job id uniquely identifies a data collection job, which contains multiple 
  // heterogeneous tasks collecting data from multiple sources. 
  string job_id = 1;

  // Multiple heterogeneous tasks this job contains.
  repeated PanopticTask tasks = 2;

  // Whether this job is a debug only job. A debug only job will not call Lambda
  // at all.
  bool debug = 3;
}

// TaskParams defines the shared/domain specific parameters that customize the 
// execution behavior.
message TaskParams {
  // If specified, append these header params to the crawler request, 
  // overwrite on same key.
  repeated KeyValuePair header_params = 1;
  
  // If specified, overwrite Cookie with the provided cookies.
  repeated KeyValuePair cookies = 2;

  // SourceId of this collected CrawlerMessage.
  string source_id = 3;

  repeated PanopticSubSource sub_sources = 4;

  // offsite of domain specific param from 20, <20 is for shared fields
  // Domain specific params that will be passed in to customize the task 
  // execution. For example, you'll pass in Weibo/Twitter/ZSXQ user id as part
  // of the task param.
  oneof params {
    JinshiTaskParams jinshi_task_params = 20;
  }
}

message TaskMetadata {
  // name of the config that triggers this task.
  string config_name = 1;

  // job_start/end_time describes the execution span of this task.
  google.protobuf.Timestamp task_start_time = 2;
  google.protobuf.Timestamp task_end_time = 3;

  // How many CrawlerMessage this task collected.
  int32 total_message_collected = 4;

  // How many CrawlerMessage this task failed to collect.
  int32 total_message_failed = 5;

  // Which ip address is this task executing at.
  string ip_addr = 6;

  // The result state of the task execution, determined by data collector.
  enum TaskResultState {
    STATE_UNSPECIFIED = 0;
    STATE_SUCCESS = 1;
    STATE_FAILURE = 2;
  }

  TaskResultState result_state = 7;
  // ...
}

// PanopticTask defines a single data collection task for a single source. A 
// task is the smallest execution in Lambda.
message PanopticTask {
  // UUID for this task.
  string task_id = 1;

  // DataCollectorId defines the data collection logic to execute.
  enum DataCollectorId {
    COLLECTOR_UNSPECIFIED = 0;
    COLLECTOR_JINSHI = 1;
    COLLECTOR_KUAILANSI = 2;
    // ...
  }

  // The mapping from this ID to corresponding collector is hard coded
  DataCollectorId data_collector_id = 2;

  // Params that customizes how to collect data from the web.
  TaskParams task_params = 3;

  // Metadata that's mostly for monitoring.
  TaskMetadata task_metadata = 4;
}

message PanopticSubSource{ 
  // example: 
  // name: 快讯
  // type: FLASHNEWS 
  // external_id: None 

  // example:
  // name: 要闻 
  // type: KEYNEWS 
  // external_id: None 

  // example:
  // name: 加州分析员 
  // type: USERS 
  // external_id:15281511824122 
  enum SubSourceType{
    UNSPECIFIED = 0;
    FLASHNEWS = 1;
    KEYNEWS = 2;
    USERS = 3;
  }

  // name of the sub source, this will be written to CrawlerMessage
  string name = 1;

  // type of the sub source, this will be used to infer:
  // 1. for SUBSOURCE_USERS: DataCollector will hard code what url to crawl
  // 1. for news : DataCollector will determine keynews or flashnews and populate specified one 
  SubSourceType type = 2;

  // this will be used to construct external request/crawl uri
  string external_id = 3;
}


// Created empty param here in case we need to pass in additional parameters to
// customize Jinshi's crawler logic.
message JinshiTaskParams {}
